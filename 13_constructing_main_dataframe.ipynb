{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BRING TOGETHER FEATURES ENGINEERED FROM POSTCODES TO CONSTRUCT MAIN DATAFRAME FOR MODELLING**\n",
    "\n",
    "The data from the Land Registry provides us with postcodes for over three hundred thousand transactions.\n",
    "\n",
    "We can now use those postcodes to engineer features for each of these properties, based on the data that has been sourced for economic factors, transport, schools, crime, and centrality.\n",
    "\n",
    "The new dataframe will then be used for predictive modelling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start building the dataframe with the cleaned up transaction data from the Land Registry, note duplicates have already been dropped\n",
    "\n",
    "df = pd.read_csv('../big_files/cleaned_transactions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import list of all London postcodes with their geo-coordinates to merge with main dataframe\n",
    "\n",
    "london_postcodes = pd.read_csv('../big_files/london_postcodes.csv')\n",
    "london_postcodes = london_postcodes.drop(columns = ['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the transaction dataframe with the geocoordinates from the postcodes\n",
    "# transform into geo-dataframe and set CRS\n",
    "\n",
    "df = pd.merge(df, london_postcodes, how = 'inner', left_on = 'postcode', right_on = 'postcode')\n",
    "df = gpd.GeoDataFrame(df, geometry = gpd.points_from_xy(df['longitude'], df['latitude']))\n",
    "df = df.set_crs(epsg = 4326)\n",
    "df = df.to_crs(epsg = 27700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shape file for all LSOA boundaries and set CRS\n",
    "\n",
    "lsoa = gpd.read_file('../big_files/LSOA_2011_London_gen_MHW.shp')\n",
    "lsoa = lsoa[['LSOA11CD', 'geometry']]\n",
    "lsoa = lsoa.set_crs(epsg = 27700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial join to merge LSOA shape file with transaction_data\n",
    "\n",
    "df = gpd.sjoin(df, lsoa, how = 'left', op = 'intersects')\n",
    "df = df.rename(columns = {'LSOA11CD': 'lsoa'})\n",
    "df = df.dropna()\n",
    "df = df.drop(columns = ['index_right'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import free school meal data and merge it with transaction dataframe\n",
    "\n",
    "school_meals = pd.read_csv('../big_files/school_meals.csv')\n",
    "school_meals = school_meals.rename(columns = {'pupil_perc_fsm_lsoa': 'fsm_lsoa'})\n",
    "school_meals = school_meals.drop(columns = ['Unnamed: 0'])\n",
    "df = pd.merge(df, school_meals, how = 'left', left_on = 'lsoa', right_on = 'lsoa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ward data to link to lsoa in transaction dataframe\n",
    "\n",
    "ward_lsoa = pd.read_csv('../big_files/ward_lsoa.csv')\n",
    "ward_lsoa = ward_lsoa.drop(columns = ['Unnamed: 0'])\n",
    "df = pd.merge(df, ward_lsoa, how = 'left', left_on = 'lsoa', right_on = 'lsoa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import crime survey data, set CRS, merge with transaction dataframe\n",
    "\n",
    "bcp = gpd.read_file('../big_files/borough_crime_perception_gdf.shp')\n",
    "bcp = bcp.drop(columns = ['GSS_CODE', 'HECTARES', 'NONLD_AREA', 'ONS_INNER', 'SUB_2009', 'SUB_2006'])\n",
    "bcp = bcp.set_crs(epsg = 27700)\n",
    "bcp = bcp.drop(columns = ['geometry'])\n",
    "df = pd.merge(df, bcp, how = 'left', left_on = 'borough', right_on = 'borough_na')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data missing from City of London area, fill with average for all boroughs\n",
    "\n",
    "df['Are gangs'] = df['Are gangs'].fillna(df['Are gangs'].mean())\n",
    "df['Are you wo'] = df['Are you wo'].fillna(df['Are you wo'].mean())\n",
    "df['How safe d'] = df['How safe d'].fillna(df['How safe d'].mean())\n",
    "df['Are you sa'] = df['Are you sa'].fillna(df['Are you sa'].mean())\n",
    "df = df.drop(columns = ['borough_na'])\n",
    "df = df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import centrality data, adjust to match length of dataframe, set CRS, add to dataframe\n",
    "\n",
    "trafalgar_square = gpd.read_file('../big_files/trafalgar_square_gdf.shp')\n",
    "trafalgar_square = trafalgar_square.reset_index(drop = True)\n",
    "trafalgar_square = trafalgar_square.drop(columns = ['FID'])\n",
    "trafalgar_square_1 = trafalgar_square.loc[:len(df) -1]\n",
    "trafalgar_square_1 = trafalgar_square_1.set_crs(epsg = 27700)\n",
    "trafalgar_square_1 = trafalgar_square_1.to_crs(epsg = 27700)\n",
    "df['dist_traf_square'] = df.distance(trafalgar_square_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import crime data grouped by LSOA, merge with final dataframe, fill those few LSOAs where there was no recorded crime\n",
    "\n",
    "grouped_crime = pd.read_csv('../big_files/grouped_crime.csv')\n",
    "grouped_crime = grouped_crime.drop(columns = ['Unnamed: 0'])\n",
    "df = pd.merge(df, grouped_crime, how = 'left', left_on = 'lsoa', right_on = 'lsoa')\n",
    "df['crime_count_lsoa'] = df['crime_count_lsoa'].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import deprivation data and merge with final dataframe\n",
    "\n",
    "deprivation = pd.read_csv('../big_files/deprivation_clean.csv')\n",
    "deprivation = deprivation.drop(columns = ['Unnamed: 0'])\n",
    "df = pd.merge(df, deprivation, how = 'left', left_on = 'lsoa', right_on = 'lsoa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gun crime data and merge with final dataframe, fill those wards that didn't record any gun crime\n",
    "\n",
    "gun = pd.read_csv('../big_files/gun.csv')\n",
    "gun = gun.loc[:, ['ward_code', 'total_gun_crimes']]\n",
    "df = pd.merge(df, gun, how = 'left', left_on = 'ward_code', right_on = 'ward_code')\n",
    "df['total_gun_crimes'] = df['total_gun_crimes'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import knife crime data and merge with final dataframe, fill missing values (City of London)\n",
    "\n",
    "knife = pd.read_csv('../big_files/knife.csv')\n",
    "knife = knife.loc[:, ['ward_code', 'total_knife_crimes']]\n",
    "df = pd.merge(df, knife, how = 'left', left_on = 'ward_code', right_on = 'ward_code')\n",
    "df['total_knife_crimes'] = df['total_knife_crimes'].fillna(df['total_knife_crimes'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge ward and lsoa data, ready to aggregate data on estate agents, delicatessens etc\n",
    "\n",
    "lsoa = lsoa.rename(columns = {'LSOA11CD': 'lsoa'})\n",
    "ward_lsoa = pd.merge(ward_lsoa, lsoa, how = 'left', left_on = 'lsoa', right_on = 'lsoa')\n",
    "ward_lsoa = gpd.GeoDataFrame(ward_lsoa, geometry = ward_lsoa['geometry'])\n",
    "ward_lsoa = ward_lsoa.set_crs(epsg = 27700)\n",
    "ward_lsoa = ward_lsoa.to_crs(epsg = 27700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import estate agent data, set CRS, merge with ward/lsoa data, groupby wards to find total/ward, merge with transaction dataframe\n",
    "# some wards don't have any estate agents\n",
    "\n",
    "estate_agents = gpd.read_file('../big_files/estate_agents_gdf.shp')\n",
    "estate_agents = estate_agents.set_crs(epsg = 4326)\n",
    "estate_agents = estate_agents.to_crs(epsg = 27700)\n",
    "estate_agents_ward = gpd.sjoin(estate_agents, ward_lsoa, how = 'left', op = 'intersects')\n",
    "ea_wards = estate_agents_ward.groupby('ward_code')['postcode'].count().reset_index()\n",
    "ea_wards = ea_wards.rename(columns = {'postcode': 'ea_count'})\n",
    "df = pd.merge(df, ea_wards, how = 'left', left_on = 'ward_code', right_on = 'ward_code')\n",
    "df['ea_count'] = df['ea_count'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import delicatessen data, merge with ward/lsoa data, groupby wards to find total/ward, merge with transaction df\n",
    "# some wards don't have any delicatessens\n",
    "\n",
    "delicatessens_gdf = gpd.read_file('../big_files/delicatessens_gdf.shp')\n",
    "delicatessens_gdf = delicatessens_gdf.drop(columns = ['name', 'Unnamed_ 0', 'latitude', 'longitude'])\n",
    "delicatessen_wards = ward_lsoa.drop(columns = ['lsoa', 'ward_name', 'borough'])\n",
    "delicatessen_wards = delicatessen_wards.drop_duplicates()\n",
    "delicatessen_wards = gpd.sjoin(delicatessen_wards, delicatessens_gdf, how = 'left', op = 'intersects')\n",
    "delicatessen_wards = delicatessen_wards.drop_duplicates(subset = ['ward_code'])\n",
    "grouped_delis = delicatessen_wards.groupby('ward_code')['postcode'].count().reset_index()\n",
    "grouped_delis = grouped_delis.rename(columns = {'postcode': 'deli_count'})\n",
    "df = pd.merge(df, grouped_delis, how = 'left', left_on = 'ward_code', right_on = 'ward_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import florist data, merge with ward/lsoa data, groupby wards to find total/ward, merge with transaction df\n",
    "# some wards don't have any florists\n",
    "\n",
    "florists = gpd.read_file('../big_files/florists.shp')\n",
    "florists_wards = ward_lsoa.drop(columns = ['lsoa', 'ward_name', 'borough'])\n",
    "florists_wards = gpd.sjoin(florists_wards, florists, how = 'left', op = 'intersects')\n",
    "florists_wards = florists_wards.drop_duplicates(subset = ['ward_code'])\n",
    "grouped_florists = florists_wards.groupby('ward_code')['postcode'].count().reset_index()\n",
    "grouped_florists = grouped_florists.rename(columns = {'postcode': 'florist_count'})\n",
    "df = pd.merge(df, grouped_florists, how = 'left', left_on = 'ward_code', right_on = 'ward_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import restaurant data, merge with ward/lsoa data, groupby wards to find total/ward, merge with transaction df\n",
    "# some wards don't have any restaurants\n",
    "\n",
    "restaurants_gdf = gpd.read_file('../big_files/restaurants_gdf.shp')\n",
    "restaurants_wards = ward_lsoa.drop(columns = ['lsoa', 'ward_name', 'borough'])\n",
    "restaurants_wards = restaurants_wards.drop_duplicates(subset = ['ward_code'])\n",
    "restaurants_wards = gpd.sjoin(restaurants_wards, restaurants_gdf, how = 'left', op = 'intersects')\n",
    "grouped_restaurants = restaurants_wards.groupby('ward_code')['postcode'].count().reset_index()\n",
    "grouped_restaurants = grouped_restaurants.rename(columns = {'postcode': 'restaurant_count'})\n",
    "df = pd.merge(df, grouped_restaurants, how = 'left', left_on = 'ward_code', right_on = 'ward_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import location data for 'outstanding' schools, including 500m radius around them\n",
    "# separate main dataframe into properties that fall inside these areas, and those that don't\n",
    "\n",
    "schools_500m = gpd.read_file('../big_files/schools_500m.shp')\n",
    "inside_500m = gpd.sjoin(df, schools_500m, how = 'inner', op = 'intersects')\n",
    "inside_500m = inside_500m.drop_duplicates(subset = ['postcode'])\n",
    "ind_500 = list(inside_500m.index)\n",
    "outside_500m = df.drop(ind_500)\n",
    "outside_500m['school_pos'] = 'None'\n",
    "df = pd.concat([inside_500m, outside_500m])\n",
    "df = df.drop(columns = ['index_right'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import location data for tube stops, including circles of various sizes around them\n",
    "\n",
    "tubes_2400m = gpd.read_file('../big_files/tubes2400m.shp')\n",
    "tubes_1600m = gpd.read_file('../big_files/tubes1600m.shp')\n",
    "tubes_800m = gpd.read_file('../big_files/tubes800m.shp')\n",
    "tubes_400m = gpd.read_file('../big_files/tubes400m.shp')\n",
    "tubes_200m = gpd.read_file('../big_files/tubes200m.shp')\n",
    "tubes_50m = gpd.read_file('../big_files/tubes50m.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataframe into those properties within 50m of tube, and those that aren't\n",
    "\n",
    "inside_50m = gpd.sjoin(df, tubes_50m, how = 'inner', op = 'intersects')\n",
    "ind_50 = list(inside_50m.index)\n",
    "outside_50m = df.drop(ind_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create segment of main dataframe between 50m and 200m from nearest tube\n",
    "#dropping duplicates as some properties are getting counted twice if they are close to two tube stops\n",
    "\n",
    "between50_200 = gpd.sjoin(outside_50m, tubes_200m, how = 'inner', op = 'intersects')\n",
    "between50_200 = between50_200.drop_duplicates(subset = ['postcode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataframe into those properties within 200m of tube, and those that aren't\n",
    "\n",
    "ind_200 = list(between50_200.index)\n",
    "outside_200m = outside_50m.drop(ind_200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create segment of main dataframe between 200m and 400m from nearest tube\n",
    "#dropping duplicates as some properties are getting counted twice if they are close to two tube stops\n",
    "\n",
    "between200_400 = gpd.sjoin(outside_200m, tubes_400m, how = 'inner', op = 'intersects')\n",
    "between200_400 = between200_400.drop_duplicates(subset = ['postcode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataframe into those properties within 400m of tube, and those that aren't\n",
    "\n",
    "ind_400 = list(between200_400.index)\n",
    "outside_400m = outside_200m.drop(ind_400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create segment of main dataframe between 400m and 800m from nearest tube\n",
    "#dropping duplicates as some properties are getting counted twice if they are close to two tube stops\n",
    "\n",
    "between400_800 = gpd.sjoin(outside_400m, tubes_800m, how = 'inner', op = 'intersects')\n",
    "between400_800 = between400_800.drop_duplicates(subset = ['postcode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataframe into those properties within 800m of tube, and those that aren't\n",
    "\n",
    "ind_800 = list(between400_800.index)\n",
    "outside_800m = outside_400m.drop(ind_800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create segment of main dataframe between 800m and 1600m from nearest tube\n",
    "#dropping duplicates as some properties are getting counted twice if they are close to two tube stops\n",
    "\n",
    "between800_1600 = gpd.sjoin(outside_800m, tubes_1600m, how = 'inner', op = 'intersects')\n",
    "between800_1600 = between800_1600.drop_duplicates(subset = ['postcode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataframe into those properties within 1600m of tube, and those that aren't\n",
    "\n",
    "ind_1600 = list(between800_1600.index)\n",
    "outside_1600m = outside_800m.drop(ind_1600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create segment of main dataframe between 1600m and 2400m from nearest tube\n",
    "#dropping duplicates as some properties are getting counted twice if they are close to two tube stops\n",
    "\n",
    "between1600_2400 = gpd.sjoin(outside_1600m, tubes_2400m, how = 'inner', op = 'intersects')\n",
    "between1600_2400 = between1600_2400.drop_duplicates(subset = ['postcode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataframe into those properties within 2400m of tube, and those that aren't\n",
    "\n",
    "ind_2400 = list(between1600_2400.index)\n",
    "outside_2400m = outside_1600m.drop(ind_2400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unecessary column from new dataframes\n",
    "\n",
    "inside_50m = inside_50m.drop(columns = ['index_right'])\n",
    "between50_200 = between50_200.drop(columns = ['index_right'])\n",
    "between200_400 = between200_400.drop(columns = ['index_right'])\n",
    "between400_800 = between400_800.drop(columns = ['index_right'])\n",
    "between800_1600 = between800_1600.drop(columns = ['index_right'])\n",
    "between1600_2400 = between1600_2400.drop(columns = ['index_right'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different cleaning for each of the new dataframes, can't write a single function\n",
    "\n",
    "inside_50m['zone_50m'] = inside_50m['zone_50m'].str[0].astype(float)\n",
    "inside_50m['zone_50m'] = inside_50m['zone_50m'].fillna(-1)\n",
    "inside_50m['zone_200m'] = -1\n",
    "inside_50m['zone_400m'] = -1\n",
    "inside_50m['zone_800m'] = -1\n",
    "inside_50m['zone_1600m'] = -1\n",
    "inside_50m['zone_2400m'] = -1\n",
    "between50_200['zone_200m'] = between50_200['zone_200m'].str[0].astype(float)\n",
    "between50_200['zone_200m'] = between50_200['zone_200m'].fillna(-1)\n",
    "between50_200['zone_50m'] = -1\n",
    "between50_200['zone_400m'] = -1\n",
    "between50_200['zone_800m'] = -1\n",
    "between50_200['zone_1600m'] = -1\n",
    "between50_200['zone_2400m'] = -1\n",
    "between200_400['zone_400m'] = between200_400['zone_400m'].str[0].astype(float)\n",
    "between200_400['zone_400m'] = between200_400['zone_400m'].fillna(-1)\n",
    "between200_400['zone_50m'] = -1\n",
    "between200_400['zone_200m'] = -1\n",
    "between200_400['zone_800m'] = -1\n",
    "between200_400['zone_1600m'] = -1\n",
    "between200_400['zone_2400m'] = -1\n",
    "between400_800['zone_800m'] = between400_800['zone_800m'].str[0].astype(float)\n",
    "between400_800['zone_800m'] = between400_800['zone_800m'].fillna(-1)\n",
    "between400_800['zone_50m'] = -1\n",
    "between400_800['zone_200m'] = -1\n",
    "between400_800['zone_400m'] = -1\n",
    "between400_800['zone_1600m'] = -1\n",
    "between400_800['zone_2400m'] = -1\n",
    "between800_1600['zone_1600m'] = between800_1600['zone_1600m'].str[0].astype(float)\n",
    "between800_1600['zone_1600m'] = between800_1600['zone_1600m'].fillna(-1)\n",
    "between800_1600['zone_50m'] = -1\n",
    "between800_1600['zone_200m'] = -1\n",
    "between800_1600['zone_400m'] = -1\n",
    "between800_1600['zone_800m'] = -1\n",
    "between800_1600['zone_2400m'] = -1\n",
    "between1600_2400['zone_2400m'] = between1600_2400['zone_2400m'].str[0].astype(float)\n",
    "between1600_2400['zone_2400m'] = between1600_2400['zone_2400m'].fillna(-1)\n",
    "between1600_2400['zone_50m'] = -1\n",
    "between1600_2400['zone_200m'] = -1\n",
    "between1600_2400['zone_400m'] = -1\n",
    "between1600_2400['zone_800m'] = -1\n",
    "between1600_2400['zone_1600m'] = -1\n",
    "outside_2400m['zone_50m'] = -1\n",
    "outside_2400m['zone_200m'] = -1\n",
    "outside_2400m['zone_400m'] = -1\n",
    "outside_2400m['zone_800m'] = -1\n",
    "outside_2400m['zone_1600m'] = -1\n",
    "outside_2400m['zone_2400m'] = -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate new dataframes into one\n",
    "\n",
    "df = pd.concat([inside_50m, between50_200, between200_400, between400_800, between800_1600, between1600_2400, outside_2400m])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final cleaning for new dataframe\n",
    "\n",
    "school = df['school_pos'] != 'None'\n",
    "no_school = df['school_pos'] == 'None'\n",
    "df.loc[school, 'school_pos'] = 1\n",
    "df.loc[no_school, 'school_pos'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tube_zone'] = 0\n",
    "zone_1 = (df['zone_50m'] == 1) | (df['zone_200m'] == 1) | (df['zone_400m'] == 1) | (df['zone_800m'] == 1) | (df['zone_1600m'] == 1) | (df['zone_2400m'] == 1)\n",
    "zone_2 = (df['zone_50m'] == 2) | (df['zone_200m'] == 2) | (df['zone_400m'] == 2) | (df['zone_800m'] == 2) | (df['zone_1600m'] == 2) | (df['zone_2400m'] == 2)\n",
    "zone_3 = (df['zone_50m'] == 3) | (df['zone_200m'] == 3) | (df['zone_400m'] == 3) | (df['zone_800m'] == 3) | (df['zone_1600m'] == 3) | (df['zone_2400m'] == 3)\n",
    "zone_4 = (df['zone_50m'] == 4) | (df['zone_200m'] == 4) | (df['zone_400m'] == 4) | (df['zone_800m'] == 4) | (df['zone_1600m'] == 4) | (df['zone_2400m'] == 4)\n",
    "zone_5 = (df['zone_50m'] == 5) | (df['zone_200m'] == 5) | (df['zone_400m'] == 5) | (df['zone_800m'] == 5) | (df['zone_1600m'] == 5) | (df['zone_2400m'] == 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[zone_1, 'tube_zone'] = 1\n",
    "df.loc[zone_2, 'tube_zone'] = 2\n",
    "df.loc[zone_3, 'tube_zone'] = 3\n",
    "df.loc[zone_4, 'tube_zone'] = 4\n",
    "df.loc[zone_5, 'tube_zone'] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['zone_50m'] = df['zone_50m'].replace(-1, 0)\n",
    "df['zone_200m'] = df['zone_200m'].replace(-1, 0)\n",
    "df['zone_400m'] = df['zone_400m'].replace(-1, 0)\n",
    "df['zone_800m'] = df['zone_800m'].replace(-1, 0)\n",
    "df['zone_1600m'] = df['zone_1600m'].replace(-1, 0)\n",
    "df['zone_2400m'] = df['zone_2400m'].replace(-1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "z50 = df['zone_50m'] != 0\n",
    "z200 = df['zone_200m'] != 0\n",
    "z400 = df['zone_400m'] != 0\n",
    "z800 = df['zone_800m'] != 0\n",
    "z1600 = df['zone_1600m'] != 0\n",
    "z2400 = df['zone_2400m'] != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[z50,'zone_50m'] = 1\n",
    "df.loc[z200,'zone_200m'] = 1\n",
    "df.loc[z400,'zone_400m'] = 1\n",
    "df.loc[z800,'zone_800m'] = 1\n",
    "df.loc[z1600,'zone_1600m'] = 1\n",
    "df.loc[z2400,'zone_2400m'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# introduce a final feature, 'any_tube' - is there any tube stop within 2400m of property?\n",
    "\n",
    "df['any_tube'] = df['zone_50m'] + df['zone_200m'] + df['zone_400m'] + df['zone_800m'] + df['zone_1600m'] + df['zone_2400m']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geographic_features = ['fsm_lsoa', 'ea_in_ward', 'avg_airbnb', 'airbnb_tot', 'crime_lsoa', 'deli_count', 'flor_count', 'rest_count', 'income_rank_pos','employment_rank_pos', 'education_rank_pos', 'health_dep_score', 'crime_rank_pos', 'housing_rank_pos', 'living_env_pos', 'zone_50m','zone_200m', 'zone_400m', 'zone_800m', 'zone_1600m', 'zone_2400m', 'dist_traf', 'gang_prob_pos', 'crime_worry_pos', 'safety_fears_pos', 'satisfaction_pos', 'gun_crime', 'knife_crime', 'good_school', 'tube_zone', 'any_tube']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns \n",
    "\n",
    "df = df.rename(columns = {'school_pos': 'good_school', 'avg_airbnb_pr_lsoa':'avg_airbnb', 'airbnb_tot_lsoa': 'airbnb_tot', 'crime_count_lsoa': 'crime_lsoa', 'Are gangs': 'gang_prob_pos', 'Are you wo': 'crime_worry_pos', 'How safe d': 'safety_fears_pos', 'Are you sa': 'satisfaction_pos', 'dist_traf_square': 'dist_traf', 'crime_count_lsoa': 'crime_lsoa', 'Income Rank (where 1 is most deprived)': 'income_rank_pos', 'Employment Rank (where 1 is most deprived)': 'employment_rank_pos', 'Education, Skills and Training Rank (where 1 is most deprived)': 'education_rank_pos', 'Health Deprivation and Disability Score': 'health_dep_score', 'Crime Rank (where 1 is most deprived)': 'crime_rank_pos', 'Barriers to Housing and Services Rank (where 1 is most deprived)':'housing_rank_pos', 'Living Environment Score': 'living_env_pos', 'school_pos': 'good_school','total_gun_crimes': 'gun_crime', 'total_knife_crimes': 'knife_crime', 'ea_count': 'ea_in_ward', 'florist_count': 'flor_count', 'restaurant_count': 'rest_count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch to categorical dtype ahead of dummifying\n",
    "\n",
    "df['zone_50m'] = df['zone_50m'].astype('category')\n",
    "df['zone_200m'] = df['zone_200m'].astype('category')\n",
    "df['zone_400m'] = df['zone_400m'].astype('category')\n",
    "df['zone_800m'] = df['zone_800m'].astype('category')\n",
    "df['zone_1600m'] = df['zone_1600m'].astype('category')\n",
    "df['zone_2400m'] = df['zone_2400m'].astype('category')\n",
    "df['good_school'] = df['good_school'].astype('category')\n",
    "df['tube_zone'] = df['tube_zone'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>date</th>\n",
       "      <th>postcode</th>\n",
       "      <th>property_type</th>\n",
       "      <th>new_build</th>\n",
       "      <th>estate_type</th>\n",
       "      <th>district</th>\n",
       "      <th>transaction_type</th>\n",
       "      <th>year</th>\n",
       "      <th>latitude</th>\n",
       "      <th>...</th>\n",
       "      <th>rest_count</th>\n",
       "      <th>good_school</th>\n",
       "      <th>zone_50m</th>\n",
       "      <th>zone_200m</th>\n",
       "      <th>zone_400m</th>\n",
       "      <th>zone_800m</th>\n",
       "      <th>zone_1600m</th>\n",
       "      <th>zone_2400m</th>\n",
       "      <th>tube_zone</th>\n",
       "      <th>any_tube</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>264450</th>\n",
       "      <td>455000</td>\n",
       "      <td>2017-03-31</td>\n",
       "      <td>E1 1BY</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>L</td>\n",
       "      <td>TOWER HAMLETS</td>\n",
       "      <td>A</td>\n",
       "      <td>2017</td>\n",
       "      <td>51.519554</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264451</th>\n",
       "      <td>380000</td>\n",
       "      <td>2018-12-12</td>\n",
       "      <td>E1 1BY</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>L</td>\n",
       "      <td>TOWER HAMLETS</td>\n",
       "      <td>A</td>\n",
       "      <td>2018</td>\n",
       "      <td>51.519554</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264557</th>\n",
       "      <td>230000</td>\n",
       "      <td>2017-05-26</td>\n",
       "      <td>E1 7AN</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>L</td>\n",
       "      <td>TOWER HAMLETS</td>\n",
       "      <td>A</td>\n",
       "      <td>2017</td>\n",
       "      <td>51.515408</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264558</th>\n",
       "      <td>425000</td>\n",
       "      <td>2017-10-27</td>\n",
       "      <td>E1 7AN</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>L</td>\n",
       "      <td>TOWER HAMLETS</td>\n",
       "      <td>A</td>\n",
       "      <td>2017</td>\n",
       "      <td>51.515408</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298045</th>\n",
       "      <td>720000</td>\n",
       "      <td>2018-04-16</td>\n",
       "      <td>E6 2JA</td>\n",
       "      <td>T</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>NEWHAM</td>\n",
       "      <td>A</td>\n",
       "      <td>2018</td>\n",
       "      <td>51.539150</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         price        date postcode property_type new_build estate_type  \\\n",
       "264450  455000  2017-03-31   E1 1BY             F         N           L   \n",
       "264451  380000  2018-12-12   E1 1BY             F         N           L   \n",
       "264557  230000  2017-05-26   E1 7AN             F         N           L   \n",
       "264558  425000  2017-10-27   E1 7AN             F         N           L   \n",
       "298045  720000  2018-04-16   E6 2JA             T         N           F   \n",
       "\n",
       "             district transaction_type  year   latitude  ...  rest_count  \\\n",
       "264450  TOWER HAMLETS                A  2017  51.519554  ...           0   \n",
       "264451  TOWER HAMLETS                A  2018  51.519554  ...           0   \n",
       "264557  TOWER HAMLETS                A  2017  51.515408  ...           0   \n",
       "264558  TOWER HAMLETS                A  2017  51.515408  ...           0   \n",
       "298045         NEWHAM                A  2018  51.539150  ...           0   \n",
       "\n",
       "       good_school zone_50m  zone_200m zone_400m zone_800m zone_1600m  \\\n",
       "264450           1      1.0        0.0       0.0       0.0        0.0   \n",
       "264451           0      1.0        0.0       0.0       0.0        0.0   \n",
       "264557           1      1.0        0.0       0.0       0.0        0.0   \n",
       "264558           0      1.0        0.0       0.0       0.0        0.0   \n",
       "298045           1      1.0        0.0       0.0       0.0        0.0   \n",
       "\n",
       "        zone_2400m  tube_zone  any_tube  \n",
       "264450         0.0          2       1.0  \n",
       "264451         0.0          2       1.0  \n",
       "264557         0.0          1       1.0  \n",
       "264558         0.0          1       1.0  \n",
       "298045         0.0          3       1.0  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_env",
   "language": "python",
   "name": "geo_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
